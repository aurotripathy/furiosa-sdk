**********************************
FuriosaAI Software Stack
**********************************

FuriosaAI provides software components in vertical so that NPU can be used on various applications and environments.
This chapter explains the role of software components and gives tutorials for usages of the components.

.. figure:: ../../../imgs/software_stack.jpg
  :alt: FuriosaAI Software Stack
  :class: with-shadow
  :width: 500px
  :align: center

The figure above represents FuriosaAI software stack on abstract layers.
The first generation of FuriosaAI NPU, :ref:`IntroToWarboy` is in the bottom of the figure.

Kernel Driver & Firmware
=============================================================
Kernel driver allows Linux OS system to identify NPU device as Linux device file.
The kernel driver needs to be reinstalled if the NPU device can not be identified on OS system.
Firmware provides low-level APIs for NPU device. Runtime and compiler controls NPU using the low-level APIs provided by firmware.
Kernel driver and firmware do not need to be exposed to users but are essential for FuriosaAI SDK.
The installation guide is available on :ref:`RequiredPackages`.

Compiler
====================================
Compiler is crucial component for both optimizing DNN models and generating executable binary on NPU.
`TFLite <https://www.tensorflow.org/lite>`_, `ONNX <https://onnx.ai/>`_ models are supported.

Furiosa compiler together with :ref:`Warboy <IntroToWarboy>` supports various ONNX operators used for vision applications.
Unsupported operators are compiled to be run on CPU.
Vision related models such as Resnet50, SSD-Mobilenet, and EfficientNet are effectively accelerated on Warboy.
If models exploit :ref:`SupportedOperators`, the models can be accelerated on Warboy as well.
As Furiosa compiler is embedded in Furiosa runtime, compiler is not necessary to be installed.
Creating session on Python/C SDK automatically exploits compiler. Compiler can be used through :ref:`CompilerCli`.

Runtime
=====================================
Runtime analyzes the executable binary program generated by compiler. Runtime also controls DNN model inference tasks.
Compiler optimizes DNN model inference and generates amount of tasks which can be executed on NPU and CPU.
Runtime finds the best way to maximally exploit HW resources and optimal schedule for the inference tasks.

Functions of runtime are provided as Python/C API. The installation guide is available on :ref:`RequiredPackages`.

Python SDK & C SDK
=====================================
Python/C SDK is a package that provides functions of runtime as API.
Object 'session' makes a certain model run on a dedicated device. Both blocking and asynchronous inferences are provided.

Python SDK or C SDK needs to be installed according to the language of applications.
Each installation guide is available :ref:`PythonSDK` and :ref:`CSDK`.

Quantizer
=====================================
:ref:`Warboy <IntroToWarboy>` supports only INT8 models.
FP32 models need to be quantized. FuriosaAI SDK provides API for quantization..
Details on quantization API is available on :ref:`ModelQuantization`.

Model Server
=====================================
Model server exposes GRPC and REST API for inference of DNN model.
`TFLite <https://www.tensorflow.org/lite>`_, `ONNX <https://onnx.ai/>`_ models contain information on tensor shapes of
input/output tensors, which enables widely used `Predict Protocol - Version 2 <https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md>`_ API.
Model server makes users to access remote API without using Python/C SDK.
Scale out can be easily implemented by using model servers and load balancer.
Model server is required to have low latency and high throughput, which can be obtained by runtime scheduling.
Installation and user guides are available on :ref:`ModelServing`.

Kubernetes Support
======================================
Kubernetes are widely used for containerized workload and service control platform.
FuriosaAI software stack also provides Kubernetes native.

Kubernetes device plugin makes Kubernetes cluster identify FuriosaAI NPU and find optimal scheduling for service.
This plugin helps resource allocation when many workloads need to be run on restricted NPUs.

Kubernetes node labeler makes physical information of NPU nodes participating on Kubernetes as meta-data.
This labeler makes users can identify physical information of NPU. Users can also select certain NPU nodes using
``spec.nodeSelector`` or ``spec.nodeAffinity`` via Kubernetes API or command line tool.

Installation and user guides are available on :ref:`KubernetesIntegration`.